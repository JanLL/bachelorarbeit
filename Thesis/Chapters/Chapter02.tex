%*****************************************
\chapter{Theoretische Grundladen}\label{ch:theoretischeGrundlagen}
%*****************************************
\section{Graphen Theorie}\label{sec:graphTheory}

Die Grundlage aller weiteren Betrachtungen ist ein Region Adjacency Graph (RAG). Um diesen zu erstellen, wird das Bild zunächst mithilfe des SLIC-Algorithmus \cite{slic} in Superpixel unterteilt, dessen Ränder möglichst an den Objektkonturen verlaufen. Das Ergebnis hiervon ist in Abb. \ref{fig:slic-rag} abgebildet. 

Der Region Adjacency Graph G baut sich aus Nodes V und Edges E auf. In unserem Fall entsprechen die Nodes den Superpixeln. Die Edges bestehen nur zwischen denjenigen Nodes, bei denen die zugehörigen Superpixel direkt angrenzen und somit eine gemeinsame Kante besitzen. 

\begin{figure}[H]
	{\includegraphics[width=.9\linewidth]{gfx/slic.png}}
\end{figure}
\captionof{figure}{Beispiel einer Superpixel Partitionierung mittels Slic}
\label{fig:slic-rag}

\vspace{0.5cm}


Bei der Segmentierung geht es darum, ein konsistentes Labeling der Superpixel zu erreichen. Dies wird über die Aktivität der Edges erreicht, welche an- oder ausgeschaltet sein können. Für die Aktivität einer Edge $y$ gilt somit: $y \in \{\text{0, 1}\}$, wobei $1$ aktiv und 0 inaktiv bedeutet \\
Konsistent ist eine Segmentierung genau dann, wenn bei aktiven Edges die zugehörigen Superpixel verschiedene Labels haben und analog bei inaktiven Edges die Superpixel die gleichen Labels. Anschaulich gesehen ist dies der Fall, wenn alle aktiven Edges geschlossene Linien bilden.


\section{Feature Space}\label{sec:featureSpace}

Der Feature Space $X \in \mathbb{R}^{|E|xD}$ ordnet jeder Edge D Features zu, die möglichst in Korrelation zur Frage stehen, ob die betrachtete Edge nun aktiv oder inaktiv sein soll. Zu den in dieser Arbeit verwendeten Feature wird in \ref{sec:exp_featureSpace} genauer eingegangen.



\section{Das Multicut Problem}\label{sec:multicutProb}

Anhand dieser, durch den Feature Space, gewichteten Edges eine konsistente Segmentierung zu erhalten wird als Multicut Problem (MP) bezeichnet. Es wird durch folgendes Minimierungsproblem beschrieben: 


\begin{equation} 
\begin{array}{rrclcl}
\displaystyle arg \min_{y_e} & \sum\limits_{y \in E} w \beta_e \cdot y \\
\textrm{s.t.} &  y - \sum\limits_{y_i \in P(y)} y_i & \le & 0 & & \forall \ y \in E
\end{array}
\end{equation}

Hierbei entspricht $w \in \mathbb{R}^D$ den Weights der einzelnen Feature und $\beta_e \in \mathbb{R}^D$ den Funktionswerten der D extrahierten Informationen des Feature Spaces. Die Nebenbedingungen erzwingen die Konsistenz der Segmentierung. $P(y)$ ist hierbei der kürzeste Pfad über inaktive Edges der beiden Superpixel, die benachbart zu $y$ sind. In der Praxis wird das Minimierungsproblem zunächst ohne Constraints gelöst und anschließend solange für diejenigen Edges hinzugefügt, die die Konsistenzbedingung verletzen, bis Konsistenz erreicht ist.

\section{Loss Funktionen}

Mithilfe einer Loss Funktion $\mathcal{L}(y, y^*)$ wird quantifiziert, wie gut eine Segmentierung $y$ mit derjenigen der Ground Truth $y^*$ übereinstimmt. In dieser Arbeit wird die Methode Variation of Information vorgestellt und mit der bestehenden des Hamming Loss verglichen.

\subsection{Hamming Loss}

\begin{equation}
\mathcal{L}(y_i, y_i^*) = \left\{ \begin{array}{lcc}  
\mathbb{I}[y_i \neq y_i^*] \cdot \alpha_{over} & \text{if} & y_i^* = 0  \\ 
\mathbb{I}[y_i \neq y_i^*] \cdot \alpha_{under} & \text{if} & y_i^* = 1         
\end{array}  \right.  \quad \forall y_i \in E 
\end{equation}

\begin{equation}
\mathcal{L}(y, y^*) = \sum\limits_{y_i \in E} \mathcal{L}(y_i, y_i^*)
\end{equation}


Es werden direkt die Edges der Segmentierung $y$ und der Ground Truth $y^*$ verglichen und bei fehlender Übereinstimmung erhöht sich der Loss. Meist ist $\alpha_{under} > \alpha_{over}$ um Übersegmentierung zu bevorzugen, da es tragischer ist Objekte nicht zu erfassen, als sie in mehreren Teilen vorzufinden. 

\subsection{Variation of Information}\label{sec:voi}

\begin{equation}
\mathcal{L}(y, y^*) = H_y + H_{y^*} - 2 \cdot I(y, y^*)
\end{equation}

$H_x$ ist hierbei die Entropie der Segmentierung $x$. Jede Segmentierung besitzt eine individuelle Entropie.  \\
$I(x, x^*)$ bezeichnet die Transinformation, anschaulich gesehen entspricht diese der Schnittmenge der Ist- und Soll-Segmentierung. 
Es werden also die Labels der Superpixel untersucht und bei fehlender Übereinstimmung beim Vergleich mit der Ground Truth erhöht sich der Loss.

\section{Structured Learning}\label{sec:strucLearn}

Um später mithilfe des Multicut Algorithmus Bilder optimal segmentieren zu können, muss der Parameter $w$ bestimmt werden. "Optimal" bedeutet hier in Bezug auf eine Loss Funktion, die als Qualitätskriterium dient. Da ein niedriger Loss für eine gute Segmentierung steht ist also das folgende Minimierungsproblem zu lösen:

\begin{equation}
\hat{w} = arg\min_{w} \mathcal{L}(y, y^*)
\end{equation}

\subsection{Subgradient Descent}

Der Subgradient Descent Algorithmus basiert auf der Berechnung der Differenz der akkumulierten Feature der Segmentierung $y$ und der Ground Truth $y^*$, welche gewichtet dem weight Vector $w$ hinzuaddiert werden. Für nähere Details siehe \cite{NowozinStrucLearn11}.

Die Minimierung des Partition Hamming Losses in dieser Arbeit wird hiermit realisiert.

\subsection{Stochastic Gradient}

Der hier verwendete Stochastic Gradient ist eine Variante des in \cite{NowozinStrucLearn11} näher erläuterten Algorithmus. Im folgenden wird die hier angewandte Methode zur Ermittlung der Gradientenrichtung (Alg. \ref{alg:sg_dir}), sowie der Liniensuche (Alg. \ref{alg:sgd_ls}), also der Schrittweite pro Iterationsschritt beschrieben: \\

\begin{algorithm}[H]
\caption{Get Gradient Descent Direction}\label{alg:sg_dir}
\begin{algorithmic}[1]
\Procedure{GetGradientDescentDirection($\#Perturbs, \sigma$, $w$)}{}
	\State $\sigma$: Noise standard deviation
	\State $w$: Current Weight Wector
	\State
	\State $\Delta x = 0$
	\For{$n=1 ... \#Perturbs$}
		\State Generate Noise $\in \mathcal{N}(0, \sigma^2)$ und add to $w$
		\State Calculate Loss on current Training Sample
		\State $\Delta x = \Delta x + Noise*Loss$
	\EndFor
	\State $\Delta x = -\Delta x / \#Perturbs$
	\State \Return{$\Delta x$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Um die Gradientenrichtung zu bestimmen wird zunächst in \#Perturbs verschiedene normalverteilte Richtungen, vom momentanen Weight Vector aus, der Loss auf dem aktuellen Bild berechnet. Anschließend werden die einzelnen Richtungen nach ihrem Loss gewichtet, wodurch man eine Richtung starken Anstieges ermittelt hat. Daher ist am Ende noch ein Vorzeichenwechsel nötig.

\vspace{1cm}


\begin{algorithm}[H]
\caption{Line Search and update Weights}\label{alg:sgd_ls}
\begin{algorithmic}[1]
\Procedure{LineSearchAndTakeStep($w, \eta, \Delta x$)}{}
	\State $\eta$: Stepwidth
	\State $w$: Current weight vector
	\State $\Delta x$: Gradient Descent Direction
	\State
	\For{$n=\{0.1, 0.5, 1.0, 5.0, 10.0\}$}
		\State Varied Weight Vector $w_{var} = w + \Delta x \cdot n$
		\State Calculate mean Loss $\mathcal{L}$ on entire Training Set
		\State from $w_{var}$
		\If{$\mathcal{L} < \mathcal{L}_{best}$}
			\State $\mathcal{L}_{best} = \mathcal{L}$
			\State Save $w_{best} = w$
			\State Break for-loop
		\EndIf
		\State Memorize $\mathcal{L}$ and associated varied Weight Vector
	\EndFor
	\State $w = w_{var}$, where regarding Loss is minimal
	\State \Return $w$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Für äquidistant gewählte Schrittweiten über 2 Größenordnungen wird jeweils der Mittelwert des Losses auf dem gesamten Trainingsset ermittelt und mit dem bisher besten Wert verglichen. Bei Erreichen eines neuen Tiefpunktes, wird direkt dorthin gesprungen, andernfalls wird die Schrittweite mit dem Niedrigsten erreichten Loss gewählt.







%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
